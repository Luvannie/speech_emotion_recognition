{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7317287,"sourceType":"datasetVersion","datasetId":4207118}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nhtanhnguyn0029/crema-fillna-mean?scriptVersionId=157388230\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU,Attention\nfrom keras.preprocessing.text import Tokenizer\n# from keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import SGD\nimport warnings\nimport pickle\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nimport tensorflow as tf \nprint (\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:46.732057Z","iopub.execute_input":"2024-01-01T15:18:46.732495Z","iopub.status.idle":"2024-01-01T15:18:46.74367Z","shell.execute_reply.started":"2024-01-01T15:18:46.732448Z","shell.execute_reply":"2024-01-01T15:18:46.742459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Emotions = pd.read_csv('/kaggle/input/speech-emotion/emotion_crema.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:46.745361Z","iopub.execute_input":"2024-01-01T15:18:46.745719Z","iopub.status.idle":"2024-01-01T15:18:51.208941Z","shell.execute_reply.started":"2024-01-01T15:18:46.745691Z","shell.execute_reply":"2024-01-01T15:18:51.207918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Emotions.isna().any())","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:51.210158Z","iopub.execute_input":"2024-01-01T15:18:51.210464Z","iopub.status.idle":"2024-01-01T15:18:51.234056Z","shell.execute_reply.started":"2024-01-01T15:18:51.210439Z","shell.execute_reply":"2024-01-01T15:18:51.233208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Emotions.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:51.236104Z","iopub.execute_input":"2024-01-01T15:18:51.236384Z","iopub.status.idle":"2024-01-01T15:18:51.257363Z","shell.execute_reply.started":"2024-01-01T15:18:51.236361Z","shell.execute_reply":"2024-01-01T15:18:51.256563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=Emotions.drop('emotion',axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:51.25847Z","iopub.execute_input":"2024-01-01T15:18:51.258792Z","iopub.status.idle":"2024-01-01T15:18:51.307379Z","shell.execute_reply.started":"2024-01-01T15:18:51.258767Z","shell.execute_reply":"2024-01-01T15:18:51.306367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Emotions=Emotions.fillna(X.mean())\nprint(Emotions.isna().any())\nEmotions.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:51.308689Z","iopub.execute_input":"2024-01-01T15:18:51.309056Z","iopub.status.idle":"2024-01-01T15:18:52.822364Z","shell.execute_reply.started":"2024-01-01T15:18:51.309024Z","shell.execute_reply":"2024-01-01T15:18:52.821465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Emotions.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:52.823728Z","iopub.execute_input":"2024-01-01T15:18:52.824087Z","iopub.status.idle":"2024-01-01T15:18:52.861798Z","shell.execute_reply.started":"2024-01-01T15:18:52.824054Z","shell.execute_reply":"2024-01-01T15:18:52.860826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(Emotions.isna())","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:18:52.86325Z","iopub.execute_input":"2024-01-01T15:18:52.863623Z","iopub.status.idle":"2024-01-01T15:18:53.011759Z","shell.execute_reply.started":"2024-01-01T15:18:52.863589Z","shell.execute_reply":"2024-01-01T15:18:53.010768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Prepare","metadata":{}},{"cell_type":"code","source":"Y=Emotions['emotion']\nX= Emotions.drop('emotion',axis= 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:12.587143Z","iopub.execute_input":"2024-01-01T15:19:12.587848Z","iopub.status.idle":"2024-01-01T15:19:12.78084Z","shell.execute_reply.started":"2024-01-01T15:19:12.587811Z","shell.execute_reply":"2024-01-01T15:19:12.779959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:12.782824Z","iopub.execute_input":"2024-01-01T15:19:12.783243Z","iopub.status.idle":"2024-01-01T15:19:12.789814Z","shell.execute_reply.started":"2024-01-01T15:19:12.783208Z","shell.execute_reply":"2024-01-01T15:19:12.788907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:12.790943Z","iopub.execute_input":"2024-01-01T15:19:12.79124Z","iopub.status.idle":"2024-01-01T15:19:12.802741Z","shell.execute_reply.started":"2024-01-01T15:19:12.791208Z","shell.execute_reply":"2024-01-01T15:19:12.80181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:12.803959Z","iopub.execute_input":"2024-01-01T15:19:12.804287Z","iopub.status.idle":"2024-01-01T15:19:12.819392Z","shell.execute_reply.started":"2024-01-01T15:19:12.804262Z","shell.execute_reply":"2024-01-01T15:19:12.818271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('encoder.pkl', 'wb') as f:\n    pickle.dump(encoder, f)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:12.822643Z","iopub.execute_input":"2024-01-01T15:19:12.822996Z","iopub.status.idle":"2024-01-01T15:19:12.828234Z","shell.execute_reply.started":"2024-01-01T15:19:12.822955Z","shell.execute_reply":"2024-01-01T15:19:12.827223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train test split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=35,test_size=0.2, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:12.82964Z","iopub.execute_input":"2024-01-01T15:19:12.829992Z","iopub.status.idle":"2024-01-01T15:19:13.134745Z","shell.execute_reply.started":"2024-01-01T15:19:12.82996Z","shell.execute_reply":"2024-01-01T15:19:13.133828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reshape for lstm\nX_train = x_train.to_numpy().reshape(x_train.shape[0] , x_train.shape[1] , 1)\nX_test = x_test.to_numpy().reshape(x_test.shape[0] , x_test.shape[1] , 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.135868Z","iopub.execute_input":"2024-01-01T15:19:13.136175Z","iopub.status.idle":"2024-01-01T15:19:13.211996Z","shell.execute_reply.started":"2024-01-01T15:19:13.136135Z","shell.execute_reply":"2024-01-01T15:19:13.211116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.213138Z","iopub.execute_input":"2024-01-01T15:19:13.213429Z","iopub.status.idle":"2024-01-01T15:19:13.856965Z","shell.execute_reply.started":"2024-01-01T15:19:13.213405Z","shell.execute_reply":"2024-01-01T15:19:13.856038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.860177Z","iopub.execute_input":"2024-01-01T15:19:13.860475Z","iopub.status.idle":"2024-01-01T15:19:13.866236Z","shell.execute_reply.started":"2024-01-01T15:19:13.860448Z","shell.execute_reply":"2024-01-01T15:19:13.865148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('best_model_local_weights.h5', monitor='val_accuracy', save_best_only=True)\nearly_stop=EarlyStopping(monitor='val_acc',mode='auto',patience=5,restore_best_weights=True)\nlr_reduction=ReduceLROnPlateau(monitor='val_acc',patience=5,verbose=1,factor=0.5,min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.867756Z","iopub.execute_input":"2024-01-01T15:19:13.868347Z","iopub.status.idle":"2024-01-01T15:19:13.878742Z","shell.execute_reply.started":"2024-01-01T15:19:13.868323Z","shell.execute_reply":"2024-01-01T15:19:13.877869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSTM Model","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Lambda\n# from tensorflow.keras.models import Model\n\n# # Define the input shape\n# input_seq = Input(shape=(x_train.shape[1], 1))\n\n# # Define your LSTM layers with return_sequences=True\n# lstm1 = LSTM(512, return_sequences=True)(input_seq)\n# dropout1 = Dropout(0.3)(lstm1)\n# dense1 = Dense(256, activation='relu')(dropout1)\n# lstm2 = LSTM(256, return_sequences=True)(dense1)\n# dropout2 = Dropout(0.2)(lstm2)\n# dense2 = Dense(128, activation='relu')(dropout2)\n# lstm3 = LSTM(128, return_sequences=True)(dense2)\n# dropout3 = Dropout(0.3)(lstm3)\n\n# # Define a custom Attention layer\n# def attention_layer(inputs):\n#     return tf.keras.layers.Attention()([inputs, inputs])\n\n# # Apply the custom attention layer\n# attention_output = Lambda(attention_layer)(lstm3)\n\n# # Add a final LSTM layer\n# lstm_final = LSTM(128)(attention_output)\n\n# # Add the output layer\n# output = Dense(7, activation='softmax')(lstm_final)\n\n# # Create the model\n# model_with_attention = Model(inputs=input_seq, outputs=output)\n\n# # Compile the model\n# model_with_attention.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# # Print the model summary\n# model_with_attention.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.879973Z","iopub.execute_input":"2024-01-01T15:19:13.880293Z","iopub.status.idle":"2024-01-01T15:19:13.889845Z","shell.execute_reply.started":"2024-01-01T15:19:13.880264Z","shell.execute_reply":"2024-01-01T15:19:13.888837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  hist_attention=model_with_attention.fit(X_train, y_train,\n#             epochs=50,\n#             validation_data=(X_test, y_test),batch_size=64,\n#             verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.892208Z","iopub.execute_input":"2024-01-01T15:19:13.892555Z","iopub.status.idle":"2024-01-01T15:19:13.903308Z","shell.execute_reply.started":"2024-01-01T15:19:13.892524Z","shell.execute_reply":"2024-01-01T15:19:13.902388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Accuracy of our model on test data : \" , model_with_attention.evaluate(X_test,y_test)[1]*100 , \"%\")\n# epochs = [i for i in range(40)]\n# fig , ax = plt.subplots(1,2)\n# train_acc = hist.history['accuracy']\n# train_loss = hist.history['loss']\n# test_acc = hist.history['val_accuracy']\n# test_loss = hist.history['val_loss']\n\n# fig.set_size_inches(20,6)\n# ax[0].plot(epochs , train_loss , label = 'Training Loss')\n# ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n# ax[0].set_title('Training & Testing Loss')\n# ax[0].legend()\n# ax[0].set_xlabel(\"Epochs\")\n\n# ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n# ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n# ax[1].set_title('Training & Testing Accuracy')\n# ax[1].legend()\n# ax[1].set_xlabel(\"Epochs\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.904598Z","iopub.execute_input":"2024-01-01T15:19:13.904912Z","iopub.status.idle":"2024-01-01T15:19:13.913911Z","shell.execute_reply.started":"2024-01-01T15:19:13.904883Z","shell.execute_reply":"2024-01-01T15:19:13.913063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # predicting on test data.\n# pred_test0 = model_with_attention.predict(x_test)\n# y_pred0 = encoder.inverse_transform(pred_test0)\n# y_test0 = encoder.inverse_transform(y_test)\n\n# # Check for random predictions\n# df0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n# df0['Predicted Labels'] = y_pred0.flatten()\n# df0['Actual Labels'] = y_test0.flatten()\n\n# df0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.917723Z","iopub.execute_input":"2024-01-01T15:19:13.917988Z","iopub.status.idle":"2024-01-01T15:19:13.92775Z","shell.execute_reply.started":"2024-01-01T15:19:13.917958Z","shell.execute_reply":"2024-01-01T15:19:13.926885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN model\n","metadata":{}},{"cell_type":"code","source":"#Reshape for CNN_LSTM MODEL\n\nx_traincnn =np.expand_dims(x_train, axis=2)\nx_testcnn= np.expand_dims(x_test, axis=2)\nx_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape\n#x_testcnn[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:19:13.928723Z","iopub.execute_input":"2024-01-01T15:19:13.928985Z","iopub.status.idle":"2024-01-01T15:19:13.941703Z","shell.execute_reply.started":"2024-01-01T15:19:13.928964Z","shell.execute_reply":"2024-01-01T15:19:13.940791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel = tf.keras.Sequential([\n    L.Conv1D(512,kernel_size=5, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(512,kernel_size=5,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(256,kernel_size=5,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(256,kernel_size=3,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(128,kernel_size=3,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n#     L.Dense(512,activation='relu'),\n#     L.BatchNormalization(),\n    L.Dense(6,activation='softmax')\n])\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:26:15.003922Z","iopub.execute_input":"2024-01-01T15:26:15.004949Z","iopub.status.idle":"2024-01-01T15:26:15.319482Z","shell.execute_reply.started":"2024-01-01T15:26:15.004916Z","shell.execute_reply":"2024-01-01T15:26:15.318545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:26:15.321346Z","iopub.execute_input":"2024-01-01T15:26:15.321666Z","iopub.status.idle":"2024-01-01T15:39:40.479715Z","shell.execute_reply.started":"2024-01-01T15:26:15.321638Z","shell.execute_reply":"2024-01-01T15:39:40.478713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:39:40.48134Z","iopub.execute_input":"2024-01-01T15:39:40.481802Z","iopub.status.idle":"2024-01-01T15:39:42.759283Z","shell.execute_reply.started":"2024-01-01T15:39:40.481768Z","shell.execute_reply":"2024-01-01T15:39:42.758415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test0 = model.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:39:42.76105Z","iopub.execute_input":"2024-01-01T15:39:42.76134Z","iopub.status.idle":"2024-01-01T15:39:44.117389Z","shell.execute_reply.started":"2024-01-01T15:39:42.761315Z","shell.execute_reply":"2024-01-01T15:39:44.11636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel1 = tf.keras.Sequential([\n    L.Conv1D(128,kernel_size=100, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(128,kernel_size=100,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(128,kernel_size=50,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(128,kernel_size=50,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(128,kernel_size=30,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n#     L.Dense(512,activation='relu'),\n#     L.BatchNormalization(),\n    L.Dense(6,activation='softmax')\n])\nmodel1.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel1.summary()\n\nhistory=model1.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])\n\nprint(\"Accuracy of our model on test data : \" , model1.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n# predicting on test data.\npred_test0 = model1.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:42:33.133742Z","iopub.execute_input":"2024-01-01T15:42:33.134694Z","iopub.status.idle":"2024-01-01T15:50:12.646391Z","shell.execute_reply.started":"2024-01-01T15:42:33.134656Z","shell.execute_reply":"2024-01-01T15:50:12.645309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel2 = Sequential()\nmodel2.add(Conv1D(256, 5,padding='same', input_shape=(X_train.shape[1],1)))\nmodel2.add(Activation('relu'))\nmodel2.add(Conv1D(128, 5,padding='same'))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.1))\nmodel2.add(MaxPooling1D(pool_size=(8)))\nmodel2.add(Conv1D(128, 5,padding='same',))\nmodel2.add(Activation('relu'))\nmodel2.add(Conv1D(128, 5,padding='same',))\nmodel2.add(Activation('relu'))\nmodel2.add(Conv1D(128, 5,padding='same',))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Conv1D(128, 5,padding='same',))\nmodel2.add(Activation('relu'))\nmodel2.add(Flatten())\nmodel2.add(Dense(6))\nmodel2.add(Activation('softmax'))\nopt = keras.optimizers.RMSprop(learning_rate=0.00001)\n\n# Plotting Model Summary\n\nmodel2.summary()\n\n# Compile your model\n\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nhistory=model1.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])\n\nprint(\"Accuracy of our model on test data : \" , model2.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n# predicting on test data.\npred_test0 = model2.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:04:24.568084Z","iopub.execute_input":"2024-01-01T16:04:24.569016Z","iopub.status.idle":"2024-01-01T16:11:52.61881Z","shell.execute_reply.started":"2024-01-01T16:04:24.568982Z","shell.execute_reply":"2024-01-01T16:11:52.617444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel3 = tf.keras.Sequential([\n    L.Conv1D(128,kernel_size=50, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(128,kernel_size=50,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(64,kernel_size=30,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(64,kernel_size=30,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(32,kernel_size=15,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n#     L.Dense(512,activation='relu'),\n#     L.BatchNormalization(),\n    L.Dense(6,activation='softmax')\n])\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel3.summary()\n\nhistory=model3.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])\n\nprint(\"Accuracy of our model on test data : \" , model3.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n# predicting on test data.\npred_test0 = model3.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:16:37.749129Z","iopub.execute_input":"2024-01-01T16:16:37.749953Z","iopub.status.idle":"2024-01-01T16:21:16.936894Z","shell.execute_reply.started":"2024-01-01T16:16:37.749917Z","shell.execute_reply":"2024-01-01T16:21:16.935989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel3 = tf.keras.Sequential([\n    L.Conv1D(128,kernel_size=30, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(128,kernel_size=30,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(64,kernel_size=30,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(32,kernel_size=30,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(64,kernel_size=10,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n#     L.Dense(512,activation='relu'),\n#     L.BatchNormalization(),\n    L.Dense(6,activation='softmax')\n])\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel3.summary()\n\nhistory=model3.fit(x_traincnn, y_train, epochs=300, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])\n\nprint(\"Accuracy of our model on test data : \" , model3.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n# predicting on test data.\npred_test0 = model3.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:30:51.771493Z","iopub.execute_input":"2024-01-01T16:30:51.772245Z","iopub.status.idle":"2024-01-01T16:38:56.545403Z","shell.execute_reply.started":"2024-01-01T16:30:51.772205Z","shell.execute_reply":"2024-01-01T16:38:56.543738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel3 = tf.keras.Sequential([\n    L.Conv1D(128,kernel_size=50, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(128,kernel_size=50,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(64,kernel_size=30,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(64,kernel_size=30,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(32,kernel_size=15,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n#     L.Dense(512,activation='relu'),\n#     L.BatchNormalization(),\n    L.Dense(6,activation='softmax')\n])\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel3.summary()\n\nhistory=model3.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])\n\nprint(\"Accuracy of our model on test data : \" , model3.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n# predicting on test data.\npred_test0 = model3.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:39:34.624486Z","iopub.execute_input":"2024-01-01T16:39:34.624864Z","iopub.status.idle":"2024-01-01T16:43:21.707287Z","shell.execute_reply.started":"2024-01-01T16:39:34.624832Z","shell.execute_reply":"2024-01-01T16:43:21.706185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel3 = tf.keras.Sequential([\n    L.Conv1D(256,kernel_size=50, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(256,kernel_size=50,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(128,kernel_size=30,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(64,kernel_size=30,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(32,kernel_size=15,strides=1,padding='same',activation='relu'),\n#     L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n#     L.Dense(512,activation='relu'),\n#     L.BatchNormalization(),\n    L.Dense(6,activation='softmax')\n])\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\nmodel3.summary()\n\nhistory=model3.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=128,callbacks=[early_stop,lr_reduction,model_checkpoint])\n\nprint(\"Accuracy of our model on test data : \" , model3.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n# predicting on test data.\npred_test0 = model3.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:43:31.344397Z","iopub.execute_input":"2024-01-01T16:43:31.344784Z","iopub.status.idle":"2024-01-01T16:53:56.864079Z","shell.execute_reply.started":"2024-01-01T16:43:31.344754Z","shell.execute_reply":"2024-01-01T16:53:56.863077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}